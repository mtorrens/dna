---
title: "Locating protein-creating genes by parsing DNA sequences with the Viterbi algorithm"
author: "Roger CuscÃ³, Matthew Sudmann-Day and Miquel Torrens"
date: "April 1, 2016"
output:
  beamer_presentation:
    toc: false
    incremental: false
    slide_level: 2
    highlight: tango
---

## Basics

* DNA contains nucleotides: \texttt{A}, \texttt{C}, \texttt{G}, \texttt{T}

* DNA transcription:

DNA -> pre-mRNA -> mRNA -> proteins

* Not all DNA is "expressed"

## Challenge

* Exons vs. Introns

* Identify transitions between exons and introns.

## Data

* Dataset from the UCI Repository: "Molecular Biology".

* 3190 sequences of 60 letters (\texttt{AATGCCGTAT...}).

* Each sequence is labelled as (\texttt{EI}, \texttt{IE}, \texttt{N}).

* We break down each sequence into strings of 5 letters (\texttt{AAAA}, \texttt{ACGTT}...)


## Methodology

* We use a Hidden Markov Model to model the sequences.

* We predict the Hidden states using the Viterbi algorithm. Why?

```{r fig.width=3, echo=FALSE, fig.align='center'}
#```{r fig.width=1, fig.height=10, echo=FALSE}
library(png)
library(grid)
img <- readPNG("trellis.png")
grid.raster(img)
```

## Results

* Run Viterbi forward and backward and ensemble
* We chose 5-letter subsequence as it maximizes posterior success
* Run $k$-fold cross-validation but we stay with leave-one-out

Outcome:

* 82.5% in-sample success rate
* 74.1% out-of-sample success rate
    + Intron-exon transition: 69.5%
    + Exon-intron transition: 73.6%
    + Neither: 75.3%

## Conclusions

1. This algorithm can reduce exponentially the regions where scientists have to look within the DNA, predicting how useful new decoded DNA sequences can be.

2. It is a key process in gene finding, disease research and drug discovery

3. Algorithm easy to train, fairly reliable, not data-hungry and comprehensible to non-data-scientists

4. Current state-of-the-art techniques include neural networks and deep learning, which are more successful, but in some contexts HMM + Viterbi are still top-class!

